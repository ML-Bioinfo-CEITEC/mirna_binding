{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "model_train_lock.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-Bioinfo-CEITEC/mirna_binding/blob/master/notebook/model_train_lock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kqp-C2JVQzx",
        "colab_type": "text"
      },
      "source": [
        "# General Python Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8RXNyFkVQzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm.notebook import tqdm\n",
        "#from sklearn.metrics import precision_recall_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM-j2Qb9VQz3",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5WEKUYdREFR",
        "colab_type": "text"
      },
      "source": [
        "### download pre-processed ENCORI dataset\n",
        "\n",
        "The Encori Dataset was obtained by downloading the whole dataset through the\n",
        "Encori API at [URL](http://www.sysu.edu.cn/403.html).\n",
        "\n",
        "The dataset is divided into train and test set:\n",
        "\n",
        "- test set is represented by samples mapping on chromosome 1.  \n",
        "- train set is represented by any sample except those mapping on chromosome 1.  \n",
        "\n",
        "The train set should be made of a total number of 179148 samples;\n",
        "\n",
        "The below cell downloads the train set from the project folder on Github.\n",
        "\n",
        "The train set corresponds to ENCORI samples not mapping on chromosome 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFKLSi6YkKxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/ML-Bioinfo-CEITEC/mirna_binding/raw/master/data/datasets/train_set_positives.tar.xz\n",
        "!tar -xvf train_set_positives.tar.xz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dxgZfJkVQz4",
        "colab_type": "text"
      },
      "source": [
        "## General notebook functions\n",
        "\n",
        "set of functions to generate the one hot encoding version of sequences and dot matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8ki8MBIVQz5",
        "colab_type": "text"
      },
      "source": [
        "### convert input nucleotide sequences to arrays\n",
        "\n",
        "the code below takes as input a table of three columns:\n",
        "\n",
        "- genomic binding site ( 50nt length);\n",
        "- microRNA sequence (20nt length);\n",
        "- label: class [ positive or negative ];\n",
        "\n",
        "it outputs a list of arrays as:\n",
        "\n",
        "- 2d matrix of binding vs microRNA with 2 channels. first channel is watson-crick score, second channel is relative position of microRNA (or zero if not required);\n",
        "- binding site sequence as tensor of shape 50 x 4, where each channel is a nucleotide;\n",
        "- microRNA sequence as tensor of shape 20 x 4, where each channel is a nucleotide;\n",
        "- labels: numpy array [ 0 or 1, as negatie or positive];\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSyNc2e_VQz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoding(df, tensor_dim=(50,20,1)):\n",
        "    \"\"\"\n",
        "    fun transform input database to one hot encoding numpy array.\n",
        "    \n",
        "    parameters:\n",
        "    df = Pandas df with col names \"binding_sequence\", \"label\", \"mirna_binding_sequence\"\n",
        "    tensor_dim= 2d matrix shape\n",
        "    \n",
        "    output:\n",
        "    2d dot matrix, labels as np array\n",
        "    \"\"\"\n",
        "\n",
        "    # reset df indexes (needed for multithreading)\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    # alphabet for watson-crick interactions.\n",
        "    alphabet = {\"AT\": 1., \"TA\": 1., \"GC\": 1., \"CG\": 1.} \n",
        "\n",
        "    # labels to one hot encoding\n",
        "    labels = np.where(df.label == 'positive', 1., 0.)\n",
        "\n",
        "    # create empty main 2d matrix array\n",
        "    N = df.shape[0] # number of samples in df\n",
        "    shape_matrix_2d = (N, *tensor_dim) # 2d matrix shape \n",
        "    # initialize dot matrix with zeros\n",
        "    ohe_matrix_2d = np.zeros(shape_matrix_2d, dtype=\"float32\")\n",
        "\n",
        "    # compile matrix with watson-crick interactions.\n",
        "    for index, row in df.iterrows():        \n",
        "        for bind_index, bind_nt in enumerate(row.binding_sequence.upper()):\n",
        "        \n",
        "            for mirna_index, mirna_nt in enumerate(\n",
        "                row.mirna_binding_sequence.upper()\n",
        "                ):\n",
        "\n",
        "                base_pairs = bind_nt + mirna_nt\n",
        "                ohe_matrix_2d[index, bind_index, mirna_index, 0] = alphabet.get(base_pairs, 0)\n",
        "\n",
        "    return ohe_matrix_2d, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLzDFlwDVQ0G",
        "colab_type": "text"
      },
      "source": [
        "### parallelized conversion of an array/dataframe to 2D matrix\n",
        "\n",
        "The below function takes as input a Pandas df or numpy array, and split it into  batches for parallelization.\n",
        "\n",
        "Usage:\n",
        "\n",
        "`output = multithread(df, one_hot_encoding, aux=False, log=False, n_cores=24)`  \n",
        "`data = join_cores_results(output, aux=True)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uyte4-QVQ0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def join_cores_results(multithread_output):\n",
        "    \"\"\"\n",
        "    join the output of different core processes \n",
        "    \n",
        "    paramenters:\n",
        "    multithread_out=\n",
        "\n",
        "    returns:\n",
        "    concateneted array_2d_matrix and labels of all jobs\n",
        "    \"\"\"\n",
        "    array_2d_matrix = np.concatenate(\n",
        "        [ process[0] for process in multithread_output ]\n",
        "    )\n",
        "    array_labels = np.concatenate(\n",
        "    [ process[1] for process in multithread_output ]\n",
        "    )\n",
        "    return array_2d_matrix, array_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e3k_bTIVQ0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multithread(df, func, n_cores=4):\n",
        "    \"\"\"\n",
        "    split input dataset into equal parts and parallelize function\n",
        "\n",
        "    paramenters:\n",
        "    df=input Pandas dataframe\n",
        "    fun=applied function\n",
        "    n_cores=number of cores (def.4)\n",
        "    \"\"\"\n",
        "    iterable = np.array_split(df, n_cores)\n",
        "    pool = Pool(n_cores)\n",
        "    lock_func = partial(func)\n",
        "    df_update = pool.map(lock_func, iterable)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    data = join_cores_results(df_update)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyTAuqYUFdom",
        "colab_type": "text"
      },
      "source": [
        "### subsample positive class from main training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxfgAPoJFhCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positive_sample_generator(df, size, random_state=None, reduce=True):\n",
        "    \"\"\"\n",
        "    random sampling of N samples from main dataframe.\n",
        "\n",
        "    paramenters:\n",
        "    df=Pandas dataframe\n",
        "    size=number of samples to extract (int.)\n",
        "    random_state=fix seed (int., def. None)\n",
        "    reduce=remove subsamples from main df (bool, def. True)\n",
        "\n",
        "    returns:\n",
        "    positive samples as Pandas dataframe and original dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    #copy input df\n",
        "    X = df.copy()\n",
        "    positive_samples = X.sample(\n",
        "        n=size, random_state=random_state\n",
        "        ).copy()\n",
        "\n",
        "    to_drop = [ name for name in X.columns\n",
        "               if name not in [\n",
        "                    'mirna_binding_sequence', 'binding_sequence', 'label']\n",
        "               ]\n",
        "    positive_samples.drop(\n",
        "            to_drop, axis=1, inplace=True\n",
        "            )\n",
        "    if reduce:\n",
        "        df = X.drop(positive_samples.index, axis=0, inplace=False\n",
        "            ).reset_index(drop=True)\n",
        "    return positive_samples, df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qib6pgFAVQ0N",
        "colab_type": "text"
      },
      "source": [
        "### shuffle positive to create negative\n",
        "\n",
        "The function generates the negative class by creating a connection between each\n",
        "\n",
        "binding site and all mirna (expect the real one). If argument mirna_dict is\n",
        "\n",
        "provided as dictionary of mirna sequences, this dictionary will be used to\n",
        "\n",
        "create the negative class. Otherwise, all unique mirna sequences of the input\n",
        "\n",
        "df will be used to generate samples for the negative class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvf3BayqVQ0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def negative_class_generator(df, mirna_list=None, neg_ratio=None,\n",
        "                             random_state=None):\n",
        "    \"\"\"\n",
        "    random generation of negative samples.\n",
        "\n",
        "    paramenters:\n",
        "    df=Pandas dataframe\n",
        "    mirna_list=list of unique mirna sequences (list)\n",
        "    neg_ratio=number of false connections for each positive sample (int.)\n",
        "    random_state=fix seed (int.)\n",
        "\n",
        "    returns:\n",
        "    negative samples as Pandas dataframe\n",
        "    \"\"\"\n",
        "    if not mirna_list:\n",
        "        # generate mirna db of unique sequences\n",
        "        mirnadb = pd.DataFrame(\n",
        "            df.mirna_binding_sequence.unique(), columns=['mirnaid']\n",
        "        )\n",
        "    else:\n",
        "        mirnadb = pd.DataFrame(mirna_list)\n",
        "        mirnadb.columns = ['mirnaid']\n",
        "    # add mirna db to each row of df\n",
        "    connections = mirnadb.assign(key=1).merge(\n",
        "          df.assign(key=1), on='key'\n",
        "          ).drop(['key', 'label'],axis=1)\n",
        "\n",
        "    # # find index of positive connection\n",
        "    positive_samples_mask = (connections.mirnaid == \n",
        "                             connections.mirna_binding_sequence)\n",
        "    # # drop positive connection to create negative samples\n",
        "    negative_df = connections[~positive_samples_mask].copy().drop(\n",
        "      ['mirna_binding_sequence'], axis=1\n",
        "      ).reset_index(drop=True)\n",
        "    # # rename cols\n",
        "    negative_df.columns = ['mirna_binding_sequence', 'binding_sequence']\n",
        "    # # add negative labels\n",
        "    negative_df['label'] = 'negative'\n",
        "    if neg_ratio == None:\n",
        "        return negative_df\n",
        "    else:\n",
        "        neg_samples = int(df.shape[0] * neg_ratio)\n",
        "        return negative_df.sample(n=neg_samples, random_state=random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCULz4UihFyY",
        "colab_type": "text"
      },
      "source": [
        "### generate dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x8Pozw0LJ8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_subsets(\n",
        "    df=None, size=None, neg_ratio=None, pos_df=None,\n",
        "    update=False, random_state=None, mirna_list=None\n",
        "    ):\n",
        "    \"\"\"\n",
        "    complete or partial replacement of training dataset;\n",
        "\n",
        "    paramenters:\n",
        "    df=Pandas df with all positive samples (def.None)\n",
        "    size=number of samples to subset from df (int., def.None)\n",
        "    neg_ratio=retio of negatives to be generated (int., def.None)\n",
        "    pos_df=subset of positive samples (Pandas df, def.None)\n",
        "    update=complete (True) or partial (False) replacement (bool, def.False)\n",
        "    \n",
        "    returns:\n",
        "    pos_df, neg_df, df\n",
        "    \"\"\"\n",
        "    if update:\n",
        "        pos_df, df = positive_sample_generator(\n",
        "            df, size, random_state=random_state\n",
        "            )\n",
        "    neg_df = negative_class_generator(\n",
        "        pos_df, neg_ratio=neg_ratio,\n",
        "        random_state=random_state, mirna_list=mirna_list\n",
        "        )\n",
        "    return pos_df, neg_df, df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOHWPfk6VQ0W",
        "colab_type": "text"
      },
      "source": [
        "### create Hand-Picked Mini-Batches\n",
        "\n",
        "Create minibatches keeping the original positive-negative ratio. It returns a list of minibatches (pos + neg together)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7MDkyP4VQ0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_minibatches(pos_samples, neg_samples, split_batch):\n",
        "    \"\"\"\n",
        "    split positive and negative samples dataframes into minibatches. This\n",
        "    function keeps the positive vs negative ratio within each batch.\n",
        "\n",
        "    paramenters:\n",
        "    pos_samples=Pandas df with all positive samples (def.None)\n",
        "    neg_samples=Pandas df with all negative samples (def.None)\n",
        "    split_batch=split dataset into N parts (int.)\n",
        "    \n",
        "    returns:\n",
        "    A list where each element is a minibatch\n",
        "    \"\"\"\n",
        "    \n",
        "    batches_list = []\n",
        "    ## split subset_pos.class into minibatches\n",
        "    ### see numpy doc for more details:\n",
        "    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.split.html\n",
        "    batch_pos = np.array_split(pos_samples, split_batch)\n",
        "    ## split subset_neg into minibatches\n",
        "    ## of size == SPLIT_BATCH\n",
        "    batch_neg = np.array_split(neg_samples, split_batch)\n",
        "    ## zip together pos and neg subsets to create minibatches\n",
        "    for mini_index, minibatch_pairs in enumerate(zip(batch_pos, batch_neg)):\n",
        "        if log:\n",
        "            print(\n",
        "                    '### minibatch pair id is:',\n",
        "                    mini_index,\n",
        "                    'pos-neg shapes are:',\n",
        "                    minibatch_pairs[0].shape[0],\n",
        "                    minibatch_pairs[1].shape[0],\n",
        "                    sep='\\t'\n",
        "                    )\n",
        "        batch_train = pd.concat(minibatch_pairs)\n",
        "        # append each minibatch to minibatch list\n",
        "        batches_list.append(batch_train)\n",
        "    return batches_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiIebzWcVQ0Z",
        "colab_type": "text"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LruikfMULOvo",
        "colab_type": "text"
      },
      "source": [
        "### load TensorFlow and modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYP4EyTqVQ0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras as keras \n",
        "from tensorflow.keras.layers import (\n",
        "                                BatchNormalization, LeakyReLU,\n",
        "                                Input, Dense, Conv2D,\n",
        "                                MaxPooling2D, Flatten, Dropout)\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOxyo5KMVQ0a",
        "colab_type": "text"
      },
      "source": [
        "### model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2UyJlgiVQ0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_arch_00b():\n",
        "    \"\"\"\n",
        "    build model archietecture as described in the manuscript.\n",
        "\n",
        "    return a model object\n",
        "    \"\"\"\n",
        "    main_input = Input(shape=(50,20,1),\n",
        "                       dtype='float32', name='main_input'\n",
        "                       )\n",
        "\n",
        "    x = Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3),\n",
        "        padding=\"same\",\n",
        "        data_format=\"channels_last\",\n",
        "        name=\"conv_1\")(main_input)    \n",
        "    x = LeakyReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='Max_1')(x)\n",
        "    x = Dropout(rate = 0.25)(x)\n",
        "\n",
        "    x = Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        padding=\"same\",\n",
        "        data_format=\"channels_last\",\n",
        "        name=\"conv_2\")(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='Max_2')(x)\n",
        "    x = Dropout(rate = 0.25)(x)\n",
        "\n",
        "    x = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        padding=\"same\",\n",
        "        data_format=\"channels_last\",\n",
        "        name=\"conv_3\")(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='Max_3')(x)\n",
        "    x = Dropout(rate = 0.25)(x)\n",
        "\n",
        "    conv_flat = Flatten(name='2d_matrix')(x)\n",
        "\n",
        "    x = Dense(128)(conv_flat)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(rate = 0.25)(x)\n",
        "\n",
        "    x = Dense(64)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(rate = 0.25)(x)\n",
        "\n",
        "    x = Dense(32)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(rate = 0.25)(x)\n",
        "\n",
        "    main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
        "    model = Model(inputs=[main_input], outputs=[main_output], name='arch_00b')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtAFDR5cRPje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile_model():\n",
        "    K.clear_session()\n",
        "\n",
        "    model = make_arch_00b()\n",
        "    opt = Adam(\n",
        "        learning_rate=1e-3,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name=\"Adam\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss='binary_crossentropy',\n",
        "        )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPhdv5GeVQ0h",
        "colab_type": "text"
      },
      "source": [
        "### train and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDjQXXaiVQ0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(\n",
        "    model, minibatch,\n",
        "    reset_metrics=True,\n",
        "    save=False,\n",
        "    fname=None\n",
        "    ):\n",
        "    \"\"\"\n",
        "    train model on selected minibatch.\n",
        "    Doc. at https://keras.io/api/models/model_training_apis/.\n",
        "\n",
        "    paramenters:\n",
        "    model=compiled Keras model\n",
        "    minibatch=set of X_train and labels\n",
        "    reset_metrics=If True, the metrics returned will be only for this batch.\n",
        "    If False, the metrics will be statefully accumulated across batches.\n",
        "    save=save model (bool, Def.False)\n",
        "    fname=model file name\n",
        "\n",
        "    returns:\n",
        "    trained model, batch loss\n",
        "    \"\"\"\n",
        "\n",
        "    X_ohe, y_ohe = minibatch  \n",
        "    model_loss = model.train_on_batch(\n",
        "        { \"main_input\" : X_ohe},\n",
        "        { \"main_output\" : y_ohe},\n",
        "        reset_metrics=reset_metrics\n",
        "        )\n",
        "\n",
        "    if save:\n",
        "        model.save(fname)\n",
        "    return model, model_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeeSUlHF5XwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_f1(\n",
        "    model, minibatch, pos_thr=0.5, save=False, fname=None, test=False\n",
        "    ):\n",
        "    \"\"\"\n",
        "    generate F1 score for the current minibatch\n",
        "    \n",
        "    paramenters:\n",
        "    model=trained model\n",
        "    minibatch=tuple of X and labels\n",
        "    pos_thr=threshold for positive class\n",
        "    save=save model if above f1 threshold (bool, Def.False)\n",
        "    fname=model file name\n",
        "\n",
        "    returns:\n",
        "    f1 score\n",
        "    \"\"\"\n",
        "    X, y_true = minibatch\n",
        "    if test:\n",
        "        y_pred = model.predict({'main_input' : X}, batch_size=32)\n",
        "    else:\n",
        "        y_pred = model.predict_on_batch({ \"main_input\" : X})\n",
        "    \n",
        "    y_pred_class = np.where(y_pred > pos_thr, 1, 0)\n",
        "    f1 = f1_score(y_true, y_pred_class)\n",
        "    return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdB0_53PVQ0l",
        "colab_type": "text"
      },
      "source": [
        "# Run Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBG2w3qNVQ0l",
        "colab_type": "text"
      },
      "source": [
        "### load Encori Dataset\n",
        "\n",
        "Load the train set derived from ENCORI database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Qh2AnHn02E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENCORI_PATH = './train_set_positives.tsv'\n",
        "\n",
        "train_df = pd.read_csv(\n",
        "    ENCORI_PATH, sep='\\t',\n",
        "    usecols=['familyseqRepresentative', 'pos_sequence'])\n",
        "train_df['label'] = 'positive'\n",
        "train_df.columns = [\n",
        "                    'binding_sequence',\n",
        "                    'mirna_binding_sequence',\n",
        "                    'label'\n",
        "                    ]\n",
        "\n",
        "print(f'trainable encori samples are {train_df.shape}')\n",
        "train_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypOJmDFjeToS",
        "colab_type": "text"
      },
      "source": [
        "### create unique miRNA sequences database\n",
        "\n",
        "The miRNA sequences database is used to create samples of the negative class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tycyaFVEeVRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mirna_db = train_df.mirna_binding_sequence.unique().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPo0__gOeAmr",
        "colab_type": "text"
      },
      "source": [
        "### standard training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDXgpEbSenqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define number of positive samples to use for the model training\n",
        "POSITIVE_SAMPLES = 1000\n",
        "# define number of negative samples to generate for each positive sample\n",
        "NEG_RATIO = 10\n",
        "# set random state for reproducibility\n",
        "RANDOM_STATE = 1789\n",
        "np.random.seed(RANDOM_STATE)\n",
        "# set working directory (models stored here)\n",
        "WORK_DIR = \"./models/train/standard/20K_10neg/\"\n",
        "# custom model name prefix\n",
        "STRATEGY = \"standard\"\n",
        "MODEL_ID = f'model_{STRATEGY}'\n",
        "# create WORK_DIR if not exists\n",
        "if not os.path.exists(WORK_DIR):\n",
        "    os.makedirs(WORK_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpvJ-M8Nedm3",
        "colab_type": "text"
      },
      "source": [
        "#### prepare train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjw2jSRneFaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate positive and negative samples\n",
        "pos_df, neg_df, _ = generate_subsets(\n",
        "    df=train_df, size=POSITIVE_SAMPLES, neg_ratio=NEG_RATIO, pos_df=None,\n",
        "    update=True, random_state=RANDOM_STATE, mirna_list=mirna_db)\n",
        "print(f'positive df size is: {pos_df.shape}\\nnegative df size is:{neg_df.shape}')\n",
        "# concateneta positive and negative samples into a unique dataframe\n",
        "main_df = pd.concat([pos_df, neg_df])\n",
        "# convert the unique dataframe to one hot encoding\n",
        "ohe_data = multithread(\n",
        "            main_df, one_hot_encoding, n_cores=2\n",
        "            )\n",
        "\n",
        "main_df_ohe, label_ohe = ohe_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq7aDYXrlx59",
        "colab_type": "text"
      },
      "source": [
        "#### compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCAIFC6ymA9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = compile_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-_GSXqgigvS",
        "colab_type": "text"
      },
      "source": [
        "#### train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FKN17owiiO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_loss = model.fit(\n",
        "    main_df_ohe, label_ohe,\n",
        "    validation_split=0.05, epochs=10,\n",
        "    batch_size=512\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhsKDx_Av9IQ",
        "colab_type": "text"
      },
      "source": [
        "## Iterative Dynamic Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLTCvm-fsG4i",
        "colab_type": "text"
      },
      "source": [
        "### Define Iterative Train Paramenters\n",
        "This section allows the user to define train paramenters:  \n",
        "- POSITIVE_SAMPLES_SIZE: positive sample size for each iteration (int., def. 1000);\n",
        "- NEG_RATIO: starting ratio for samples of the negative class (int., def. 1);\n",
        "- RANDOM_STATE: set random state for reproducibility (int., def. 1789);\n",
        "- CORES: available cores for train dataset pre-processing to ohe (int., def. 4);\n",
        "- MINIBATCH_SPLIT: number of bins to divide the train dataset (int. def. 500);\n",
        "- WORK_DIR: save data to PATH (str., def. cwd);\n",
        "- STRATEGY: training strategy (str: iter or normal);\n",
        "- MODEL_ID: arbitrary model name (str, def. model);\n",
        "- MAX_ITER: set max number of iterations (int., def. 10);\n",
        "- POS_THR: define threshold to claim a prediction as positive (float, def. 0.5)\n",
        "- F1_TRIGGER: set threshold above that the negative ratio is increased (float, def. 0.75)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_dgDQHWVQ0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "POSITIVE_SAMPLES_SIZE = 1000\n",
        "NEG_RATIO = 1\n",
        "RANDOM_STATE = 1789\n",
        "np.random.seed(RANDOM_STATE)\n",
        "MINIBATCH_SPLIT = 10\n",
        "CORES = 2\n",
        "MAX_ITER = 600\n",
        "POS_THR = 0.7\n",
        "F1_TRIGGER = 0.7\n",
        "\n",
        "# set working directory (models stored here)\n",
        "WORK_DIR = \"./models/train/iterative/\"\n",
        "# custom model name prefix\n",
        "STRATEGY = \"iter\"\n",
        "MODEL_ID = f'model_{STRATEGY}'\n",
        "# create WORK_DIR if not exists\n",
        "if not os.path.exists(WORK_DIR):\n",
        "    os.makedirs(WORK_DIR)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA40Fm2yVQ1F",
        "colab_type": "text"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K_ry3AsVQ1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "model = make_arch_00b()\n",
        "\n",
        "opt = Adam(\n",
        "    learning_rate=1e-3,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='binary_crossentropy',\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSSTU62AVQ1I",
        "colab_type": "text"
      },
      "source": [
        "### Iterative Training\n",
        "\n",
        "This cell defines the iterative training with incresing negative ratio scheme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdmpMlh4VQ1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# copy original data frame\n",
        "main_df = train_df.copy()\n",
        "# generate mirna database\n",
        "mirna_db = main_df.mirna_binding_sequence.unique().tolist()\n",
        "# initialize array for f1 and val loss metrics\n",
        "iter_f1_score = np.zeros( (MAX_ITER, 1) )\n",
        "iter_val_losses = np.zeros( (MAX_ITER, 1) )\n",
        "# initialize negative ratio\n",
        "iter_neg_ratio = 0 + NEG_RATIO\n",
        "# tot pos samples counter\n",
        "pos_counter = 0\n",
        "# tot neg samples counter\n",
        "neg_counter = 0\n",
        "\n",
        "for iteration in tqdm(range(START_FROM_ITER, MAX_ITER), desc='iteration'):\n",
        "    # define iteration's model name\n",
        "    model_name = f'{WORK_DIR}/{iteration}.iterative.h5'\n",
        "    # evaluate f1 score\n",
        "    if (iter_f1_score[iteration - 1 ] > F1_TRIGGER:\n",
        "        # create train dataset by complete replacement\n",
        "        pos_df, neg_df, main_df = generate_subsets(\n",
        "            df=main_df, size=POSITIVE_SAMPLES_SIZE,\n",
        "            neg_ratio=iter_neg_ratio, pos_df=None,\n",
        "            update=True, random_state=RANDOM_STATE,\n",
        "            mirna_list=mirna_db)\n",
        "        pos_counter += pos_df.shape[0]\n",
        "\n",
        "    else:\n",
        "        # create train dataset by partial replacement\n",
        "        pos_df, neg_df, main_df = generate_subsets(\n",
        "            df=main_df, size=POSITIVE_SAMPLES_SIZE,\n",
        "            neg_ratio=iter_neg_ratio, pos_df=pos_df,\n",
        "            update=False, random_state=RANDOM_STATE,\n",
        "            mirna_list=mirna_db\n",
        "            )\n",
        "    # update counter of used negative samples\n",
        "    neg_counter += neg_df.shape[0]\n",
        "    print(\n",
        "        f'#\\tstart training of iteration:\\t{iteration}\\t' +\n",
        "        f'negative ratio is:\\t{iter_neg_ratio}\\t' +\n",
        "        f'total used pos-neg:\\t{pos_counter}\\t{neg_counter}\\t'\n",
        "        )\n",
        "    # prepare minibatches\n",
        "    minibatches = make_minibatches(\n",
        "        pos_df, neg_df, MINIBATCH_SPLIT\n",
        "        )\n",
        "\n",
        "    # create array where minibatch's losses are stored.\n",
        "    batch_losses = np.zeros( (MINIBATCH_SPLIT, 1), dtype='float')\n",
        "    for mini_index in tqdm(range(MINIBATCH_SPLIT), desc='minibatches'):\n",
        "        minibatch = minibatches[mini_index]\n",
        "        # minibatches to ohe generator\n",
        "        ohe_data = multithread(\n",
        "            minibatch, one_hot_encoding, n_cores=CORES\n",
        "            )\n",
        "        if mini_index == MINIBATCH_SPLIT - 1:\n",
        "            # get val loss\n",
        "            val_loss = model.test_on_batch(\n",
        "                { \"main_input\" : ohe_data[0]},\n",
        "                { \"main_output\" : ohe_data[1]}\n",
        "                )\n",
        "            iter_val_losses[iteration] = val_loss\n",
        "            # get val f1 score\n",
        "            f1 = eval_f1(\n",
        "                        model, ohe_data,\n",
        "                        f1_thr=POS_THR, save=True,\n",
        "                        fname=model_name\n",
        "                         )\n",
        "            # add val f1 score\n",
        "            iter_f1_score[iteration] = f1\n",
        "            # get minibatches average train loss\n",
        "            avg_loss = np.average(batch_losses)\n",
        "            \n",
        "            print(\n",
        "                f'train loss:\\t{round(avg_loss, 8)}\\t' +\n",
        "                f'val loss:\\t{round(val_loss, 8)}\\t' + \n",
        "                f'val f1:{round(f1, 8)}'\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            # train model\n",
        "            model, batch_loss = train( model, ohe_data, reset_metrics=True )\n",
        "            # add loss\n",
        "            batch_losses[mini_index] = batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
